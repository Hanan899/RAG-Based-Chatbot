import os
import shutil
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter

CHROMA_PATH = "chroma"
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

def create_vector_store(chunks):
    CHROMA_PATH  = "chroma"
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="chroma",
    collection_name="rag-data"
)
    return db

def load_vector_store():
    db = Chroma(
        collection_name="rag-data",
        persist_directory="chroma",
        embedding_function=embeddings
    )
    return db

def add_to_vector_store(chunks):
    db = load_vector_store()
    db.add_documents(chunks)
    return db

def query_vector_store(user_query, k=5, return_docs=True, relevance_threshold=0.5):
    print("ğŸš€ Starting query_vector_store()")
    print(f"ğŸ“¨ Incoming query: {user_query}")
    print(f"âš™ï¸ Top k = {k}, Return Docs = {return_docs}, Relevance Threshold = {relevance_threshold}")
    
    print("ğŸ“¦ Loading vector store...")
    db = load_vector_store()
    print("âœ… Vector store loaded.")

    results_final = []
    print(f"ğŸ“‚ Initialized empty results list: {results_final}")
    
    if len(user_query) > 1000:
        print(f"âœ‚ï¸ Query length = {len(user_query)} > 1000, splitting query...")
        splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        query_chunks = splitter.split_text(user_query)
        print(f"ğŸ” Query split into {len(query_chunks)} chunks.")
    else:
        query_chunks = [user_query]
        print(f"ğŸ” Query is short. Using 1 chunk: {query_chunks}")

    all_scores = []
    print("ğŸ“Š Starting similarity search for each query chunk...")

    for i, chunk in enumerate(query_chunks):
        print(f"ğŸ” Searching vector store for chunk {i+1}/{len(query_chunks)}: {chunk[:100]}...")
        results = db.similarity_search_with_score(chunk, k=k)
        print(f"ğŸ“ˆ Chunk {i+1} returned {len(results)} results with scores:")

        for doc, score in results:
            all_scores.append(score)
            print(f"   ğŸ“Œ Score: {score:.4f}")
            if return_docs:
                results_final.append(doc.page_content)
                print(f"   ğŸ“„ Content snippet: {doc.page_content[:300]}")
            else:
                results_final.append({
                    "chunk": chunk,
                    "score": score,
                    "content": doc.page_content[:300]
                })

    print(f"ğŸ“Š All scores collected: {all_scores}")
    print(f"ğŸ“¦ Total results collected: {len(results_final)}")

    if return_docs:
        print("ğŸ§ª Filtering results based on relevance threshold...")
        if not results_final:
            print("âš ï¸ No results found.")
            return []
        elif all(s <= relevance_threshold for s in all_scores):
            print(f"âŒ All scores <= {relevance_threshold} â€” considered irrelevant. Returning empty list.")
            return []
        else:
            print(f"âœ… Returning {len(results_final)} relevant documents.")
            return results_final
    else:
        print(f"âœ… Returning {len(results_final)} results with metadata.")
        return results_final
